from flask import Flask, request, jsonify, render_template, session
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import json
import io
import os
import operator
from typing import Dict, List, TypedDict, Annotated
import base64
from matplotlib.backends.backend_agg import FigureCanvasAgg
import uuid
import threading 

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# ë°ì´í„° ì²˜ë¦¬ ë° í†µê³„ ê³„ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from scipy.stats import pearsonr, ttest_ind, f_oneway, chi2_contingency
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import warnings

warnings.filterwarnings('ignore')

app = Flask(__name__)
app.secret_key = 'your-secret-key-here'  # ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ì•ˆì „í•œ í‚¤ë¡œ ë³€ê²½í•˜ì„¸ìš”

# ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì˜ ìƒíƒœì™€ ê²°ê³¼ë¥¼ ì €ì¥í•  ì¸ë©”ëª¨ë¦¬ ë”•ì…”ë„ˆë¦¬
tasks = {}


# ==============================================================================
# ë„êµ¬(Tools) ì •ì˜
# ==============================================================================
@tool
def descriptive_analysis(data_json: str, var: str) -> Dict:
    """
    ë‹¨ì¼ ë³€ìˆ˜ì— ëŒ€í•œ ê¸°ìˆ  í†µê³„(ë¹ˆë„, ë¶„í¬)ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ë³¸ê²©ì ì¸ ë¶„ì„ ì „ ë°ì´í„°ì˜ íŠ¹ì§•ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.
    :param data_json: ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„ì˜ JSON ë¬¸ìì—´.
    :param var: ë¶„ì„í•  ë³€ìˆ˜ ì´ë¦„.
    """
    try:
        data = pd.read_json(io.StringIO(data_json))
        clean_data = data[[var]].dropna()

        if pd.api.types.is_numeric_dtype(clean_data[var]):
            stats_df = clean_data[var].describe().round(2).to_dict()
            stats_df = stats_df.replace({np.nan: None})
            stats = stats_df.to_dict()
            return {
                'tool': 'descriptive_analysis',
                'variable': var,
                'variable_type': 'numeric',
                'results': {'statistics': stats},
                'plot_data': {
                    'type': 'histogram',
                    'data': clean_data[var].tolist(),
                    'title': f'Distribution of {var}',
                    'xlabel': var
                }
            }
        else:
            counts = clean_data[var].value_counts().to_dict()
            percentages = (clean_data[var].value_counts(normalize=True) * 100).round(2).to_dict()
            return {
                'tool': 'descriptive_analysis',
                'variable': var,
                'variable_type': 'categorical',
                'results': {'counts': counts, 'percentages': percentages},
                'plot_data': {
                    'type': 'bar',
                    'data': counts,
                    'title': f'Frequency of {var}',
                    'xlabel': var
                }
            }
    except Exception as e:
        return {'error': f'ê¸°ìˆ  í†µê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}


@tool
def correlation_analysis(data_json: str, var1: str, var2: str) -> Dict:
    """
    ë‘ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°„ì˜ í”¼ì–´ìŠ¨ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    :param data_json: ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„ì˜ JSON ë¬¸ìì—´.
    :param var1: ì²« ë²ˆì§¸ ë³€ìˆ˜ ì´ë¦„.
    :param var2: ë‘ ë²ˆì§¸ ë³€ìˆ˜ ì´ë¦„.
    """
    try:
        data = pd.read_json(io.StringIO(data_json))
        df_copy = data[[var1, var2]].copy()
        df_copy[var1] = pd.to_numeric(df_copy[var1], errors='coerce')
        df_copy[var2] = pd.to_numeric(df_copy[var2], errors='coerce')
        clean_data = df_copy.dropna()
        if len(clean_data) < 3:
            return {'error': 'ìƒê´€ë¶„ì„ì„ ìœ„í•œ ìœ íš¨ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.'}
        corr_coef, p_value = pearsonr(clean_data[var1], clean_data[var2])
        return {
            'tool': 'correlation_analysis',
            'variables': [var1, var2],
            'results': {'correlation_coefficient': corr_coef, 'p_value': p_value},
            'plot_data': {
                'type': 'scatter',
                'x': clean_data[var1].tolist(),
                'y': clean_data[var2].tolist(),
                'title': f'Correlation: {var1} vs {var2}',
                'xlabel': var1,
                'ylabel': var2
            }
        }
    except Exception as e:
        return {'error': f'ìƒê´€ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}


@tool
def group_comparison_analysis(data_json: str, cat_var: str, num_var: str) -> Dict:
    """
    ë²”ì£¼í˜• ë³€ìˆ˜ì— ë”°ë¥¸ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ í‰ê· ì„ ë¹„êµí•©ë‹ˆë‹¤ (t-test ë˜ëŠ” ANOVA).
    :param data_json: ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„ì˜ JSON ë¬¸ìì—´.
    :param cat_var: ê·¸ë£¹ì„ ë‚˜ëˆ„ëŠ” ë²”ì£¼í˜• ë³€ìˆ˜.
    :param num_var: í‰ê· ì„ ë¹„êµí•  ìˆ˜ì¹˜í˜• ë³€ìˆ˜.
    """
    try:
        data = pd.read_json(io.StringIO(data_json))
        df_copy = data[[cat_var, num_var]].copy()
        df_copy[num_var] = pd.to_numeric(df_copy[num_var], errors='coerce')
        clean_data = df_copy.dropna()
        groups = [group[num_var].values for name, group in clean_data.groupby(cat_var)]
        if len(groups) < 2:
            return {'error': 'ê·¸ë£¹ ë¹„êµë¥¼ ìœ„í•œ ìœ íš¨ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 2ê°œ ê·¸ë£¹ í•„ìš”).'}
        test_name, stat, pval = ("t-test", *ttest_ind(groups[0], groups[1])) if len(groups) == 2 else ("ANOVA",
                                                                                                       *f_oneway(
                                                                                                           *groups))
        return {
            'tool': 'group_comparison_analysis',
            'variables': [cat_var, num_var],
            'results': {'test_statistic': stat, 'p_value': pval, 'test_type': test_name},
            'plot_data': {
                'type': 'boxplot',
                'data': clean_data.to_dict('list'),
                'x': cat_var,
                'y': num_var,
                'title': f'{test_name}: {num_var} by {cat_var}'
            }
        }
    except Exception as e:
        return {'error': f'ê·¸ë£¹ ë¹„êµ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}


@tool
def simple_linear_regression_analysis(data_json: str, independent_var: str, dependent_var: str) -> Dict:
    """
    ë‹¨ìˆœ ì„ í˜• íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ í•˜ë‚˜ì˜ ë³€ìˆ˜ë¡œ ë‹¤ë¥¸ ë³€ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.
    :param data_json: ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„ì˜ JSON ë¬¸ìì—´.
    :param independent_var: ì˜ˆì¸¡ì— ì‚¬ìš©í•  ë…ë¦½ ë³€ìˆ˜ (xì¶•, ìˆ˜ì¹˜í˜•).
    :param dependent_var: ì˜ˆì¸¡í•˜ë ¤ëŠ” ì¢…ì† ë³€ìˆ˜ (yì¶•, ìˆ˜ì¹˜í˜•).
    """
    try:
        data = pd.read_json(io.StringIO(data_json))
        df_copy = data[[independent_var, dependent_var]].copy()
        df_copy[independent_var] = pd.to_numeric(df_copy[independent_var], errors='coerce')
        df_copy[dependent_var] = pd.to_numeric(df_copy[dependent_var], errors='coerce')
        clean_data = df_copy.dropna()
        if len(clean_data) < 3:
            return {'error': 'íšŒê·€ë¶„ì„ì„ ìœ„í•œ ìœ íš¨ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.'}
        X, y = clean_data[[independent_var]], clean_data[dependent_var]
        model = LinearRegression().fit(X, y)
        y_pred = model.predict(X)
        r_squared, coefficient, intercept = r2_score(y, y_pred), model.coef_[0], model.intercept_
        return {
            'tool': 'simple_linear_regression_analysis',
            'variables': {'independent': independent_var, 'dependent': dependent_var},
            'results': {
                'r_squared': r_squared,
                'coefficient': coefficient,
                'intercept': intercept,
                'equation': f'{dependent_var} = {coefficient:.2f} * {independent_var} + {intercept:.2f}'
            },
            'plot_data': {
                'type': 'regression',
                'x': X.values.flatten().tolist(),
                'y': y.values.flatten().tolist(),
                'y_pred': y_pred.tolist(),
                'title': f'Regression: {independent_var} vs {dependent_var}',
                'xlabel': independent_var,
                'ylabel': dependent_var
            }
        }
    except Exception as e:
        return {'error': f'íšŒê·€ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}


@tool
def chi_squared_test(data_json: str, var1: str, var2: str) -> Dict:
    """
    ë‘ ë²”ì£¼í˜• ë³€ìˆ˜ ê°„ì˜ ì—°ê´€ì„±ì´ ìˆëŠ”ì§€ ì¹´ì´ì œê³± ë…ë¦½ì„± ê²€ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    :param data_json: ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„ì˜ JSON ë¬¸ìì—´.
    :param var1: ì²« ë²ˆì§¸ ë²”ì£¼í˜• ë³€ìˆ˜.
    :param var2: ë‘ ë²ˆì§¸ ë²”ì£¼í˜• ë³€ìˆ˜.
    """
    try:
        data = pd.read_json(io.StringIO(data_json))
        df_copy = data[[var1, var2]].dropna()
        if len(df_copy) < 5:
            return {'error': 'ì¹´ì´ì œê³± ê²€ì •ì„ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.'}
        contingency_table = pd.crosstab(df_copy[var1], df_copy[var2])
        chi2, p, dof, expected = chi2_contingency(contingency_table)
        return {
            'tool': 'chi_squared_test',
            'variables': [var1, var2],
            'results': {'chi2_statistic': chi2, 'p_value': p, 'degrees_of_freedom': dof},
            'contingency_table': contingency_table.to_dict()
        }
    except Exception as e:
        return {'error': f'ì¹´ì´ì œê³± ê²€ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}


# ==============================================================================
# LangGraph ìƒíƒœ(State) ì •ì˜
# ==============================================================================
class AgentState(TypedDict):
    data_json: str
    data_profile: str
    plan: List[Dict]
    report: str
    messages: Annotated[list, operator.add]


# ==============================================================================
# ì—ì´ì „íŠ¸ ë° ë…¸ë“œ(Nodes) ì •ì˜
# ==============================================================================
class ToolCall(BaseModel):
    tool_name: str = Field(
        description="ì‚¬ìš©í•  ë„êµ¬ì˜ ì´ë¦„. ë°˜ë“œì‹œ ['descriptive_analysis', 'correlation_analysis', 'group_comparison_analysis', 'simple_linear_regression_analysis', 'chi_squared_test'] ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
    tool_args: Dict = Field(description="ë„êµ¬ì— ì „ë‹¬í•  ì¸ì ë”•ì…”ë„ˆë¦¬.")


class AnalysisPlan(BaseModel):
    tool_calls: List[ToolCall] = Field(description="ì‹¤í–‰í•  í†µê³„ ë¶„ì„ ë„êµ¬ í˜¸ì¶œ ëª©ë¡")


def create_planner_agent(llm):
    parser = PydanticOutputParser(pydantic_object=AnalysisPlan)
    prompt_template = """ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” ì „ë¬¸ ë°ì´í„° ê³¼í•™ìì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë°ì´í„° í”„ë¡œí•„ì„ ê¸°ë°˜ìœ¼ë¡œ, ë°ì´í„°ì˜ í•µì‹¬ì ì¸ íŠ¹ì§•ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ìˆ˜í–‰í•´ì•¼ í•  í†µê³„ ë¶„ì„ ë„êµ¬ë“¤ì˜ ëª©ë¡ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.
ë¨¼ì € ì£¼ìš” ë³€ìˆ˜ë“¤ì— ëŒ€í•´ 'descriptive_analysis'ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ì˜ ê¸°ë³¸ì ì¸ ë¶„í¬ì™€ íŠ¹ì§•ì„ íŒŒì•…í•˜ëŠ” ê²ƒì„ ìš°ì„ ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”.
ê·¸ í›„, ë³€ìˆ˜ë“¤ì˜ ê´€ê³„ë¥¼ ë” ê¹Šì´ íŒŒì•…í•  ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ë¶„ì„ë“¤ì„ ê³„íší•˜ì„¸ìš”. ê° ë¶„ì„ì€ ë°˜ë“œì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
- descriptive_analysis(var: str): ë‹¨ì¼ ë³€ìˆ˜ì˜ ë¶„í¬(ìˆ˜ì¹˜í˜•)ë‚˜ ë¹ˆë„(ë²”ì£¼í˜•)ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. ê°€ì¥ ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
- correlation_analysis(var1: str, var2: str): ë‘ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°„ì˜ ìƒê´€ê´€ê³„ ë¶„ì„.
- group_comparison_analysis(cat_var: str, num_var: str): ë²”ì£¼í˜• ë³€ìˆ˜ì— ë”°ë¥¸ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ í‰ê·  ë¹„êµ.
- simple_linear_regression_analysis(independent_var: str, dependent_var: str): í•˜ë‚˜ì˜ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¡œ ë‹¤ë¥¸ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ ìƒì„±.
- chi_squared_test(var1: str, var2: str): ë‘ ë²”ì£¼í˜• ë³€ìˆ˜ ê°„ì˜ ì—°ê´€ì„± ê²€ì •.

{format_instructions}

ë°ì´í„° í”„ë¡œí•„:
{data_profile}
"""

    def planner_node(state: AgentState):
        profile_str = json.dumps(state['data_profile'], indent=2, ensure_ascii=False)
        prompt = prompt_template.format(data_profile=profile_str, format_instructions=parser.get_format_instructions())
        response = llm.invoke(prompt)
        try:
            parsed_plan = parser.parse(response.content)
            plan = [call.dict() for call in parsed_plan.tool_calls]
            return {"plan": plan}
        except Exception as e:
            return {"plan": []}

    return planner_node


tools = [descriptive_analysis, correlation_analysis, group_comparison_analysis, simple_linear_regression_analysis,
         chi_squared_test]
statistics_expert_node = ToolNode(tools)


def create_reporter_agent(llm):
    prompt = """ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ëŠ” ì—¬ëŸ¬ í†µê³„ ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ì—¬ ì–»ì€ ê²°ê³¼ë“¤ì˜ ëª©ë¡ì…ë‹ˆë‹¤. ê° ê²°ê³¼ë¥¼ í•´ì„í•˜ê³ , í†µê³„ì  ìœ ì˜ì„±ì„ ì„¤ëª…í•˜ë©°,
ë°ì´í„°ë¡œë¶€í„° ì–»ì„ ìˆ˜ ìˆëŠ” ì „ë°˜ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¢…í•©í•˜ì—¬ í•˜ë‚˜ì˜ ì™„ì„±ëœ ë³´ê³ ì„œë¥¼ í•œêµ­ì–´ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë¶„ì„ ê²°ê³¼ ëª©ë¡:
{analysis_results}
"""

    def reporter_node(state: AgentState):
        tool_outputs = [json.loads(msg.content) for msg in state['messages'] if isinstance(msg, ToolMessage)]
        results_str = json.dumps(tool_outputs, indent=2, ensure_ascii=False)
        response = llm.invoke(prompt.format(analysis_results=results_str))
        return {"report": response.content}

    return reporter_node


def prepare_next_step_node(state: AgentState):
    if len(state.get('plan', [])) > sum(isinstance(msg, ToolMessage) for msg in state['messages']):
        next_tool_index = sum(isinstance(msg, ToolMessage) for msg in state['messages'])
        next_tool_call = state['plan'][next_tool_index]
        return {"messages": [AIMessage(content="", tool_calls=[
            {"id": f"tool_{next_tool_call['tool_name']}_{next_tool_index}", "name": next_tool_call['tool_name'],
             "args": {"data_json": state['data_json'], **next_tool_call['tool_args']}}])]}
    else:
        return {}


def route_after_prepare(state: AgentState):
    if len(state.get('plan', [])) > sum(isinstance(msg, ToolMessage) for msg in state['messages']):
        return "statistics_expert"
    else:
        return "reporter"


# ==============================================================================
# ê·¸ë˜í”„(Graph) ìƒì„± ë° ì—°ê²°
# ==============================================================================

def create_graph(llm, for_planning_only=False):
    graph_builder = StateGraph(AgentState)

    if for_planning_only:
        planner_node = create_planner_agent(llm)
        graph_builder.add_node("planner", planner_node)
        graph_builder.set_entry_point("planner")
        graph_builder.add_edge("planner", END)
    else:
        reporter_node = create_reporter_agent(llm)
        graph_builder.add_node("prepare_next_step", prepare_next_step_node)
        graph_builder.add_node("statistics_expert", statistics_expert_node)
        graph_builder.add_node("reporter", reporter_node)
        graph_builder.set_entry_point("prepare_next_step")
        graph_builder.add_edge("statistics_expert", "prepare_next_step")
        graph_builder.add_conditional_edges("prepare_next_step", route_after_prepare,
                                            {"statistics_expert": "statistics_expert", "reporter": "reporter"})
        graph_builder.add_edge("reporter", END)

    return graph_builder.compile()


# ==============================================================================
# ì°¨íŠ¸ ìƒì„± ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==============================================================================

def create_plot_base64(plot_data: Dict) -> str:
    """ì°¨íŠ¸ë¥¼ ìƒì„±í•˜ê³  base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë¬¸ìì—´ì„ ë°˜í™˜"""
    fig, ax = plt.subplots(figsize=(10, 6))
    plot_type = plot_data.get('type')

    try:
        if plot_type == 'scatter':
            ax.scatter(plot_data['x'], plot_data['y'], alpha=0.6)
        elif plot_type == 'boxplot':
            data_df = pd.DataFrame(plot_data['data'])
            data_df.boxplot(column=plot_data['y'], by=plot_data['x'], ax=ax)
            plt.suptitle('')
        elif plot_type == 'histogram':
            ax.hist(plot_data['data'], bins=30, alpha=0.7)
        elif plot_type == 'bar':
            bar_data = plot_data['data']
            if len(bar_data) > 20:
                bar_data = dict(list(bar_data.items())[:20])
                ax.set_title(f"Top 20 {plot_data.get('title', '')}")
            else:
                ax.set_title(plot_data.get('title', ''))
            ax.bar(bar_data.keys(), bar_data.values())
            plt.xticks(rotation=45, ha='right')
        elif plot_type == 'regression':
            ax.scatter(plot_data['x'], plot_data['y'], alpha=0.6, label='Actual Data')
            ax.plot(plot_data['x'], plot_data['y_pred'], color='red', linewidth=2, label='Regression Line')
            ax.legend()

        ax.set_title(plot_data.get('title', ''))
        ax.set_xlabel(plot_data.get('xlabel', ''))
        ax.set_ylabel(plot_data.get('ylabel', ''))
        plt.tight_layout()

        buffer = io.BytesIO()
        canvas = FigureCanvasAgg(fig)
        canvas.print_png(buffer)
        buffer.seek(0)
        image_png = buffer.getvalue()
        buffer.close()
        plt.close(fig)

        graphic = base64.b64encode(image_png)
        graphic = graphic.decode('utf-8')
        return graphic

    except Exception as e:
        plt.close(fig)
        return None


# ==============================================================================
# ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜
# ==============================================================================
def run_analysis_in_background(task_id, api_key, selected_plan, df_json):
    """ì‹¤ì œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ tasks ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    try:
        global tasks
        tasks[task_id]['status'] = 'running'

        os.environ["OPENAI_API_KEY"] = api_key
        llm = ChatOpenAI(model="gpt-4o")

        execution_graph = create_graph(llm, for_planning_only=False)
        initial_state = {
            "data_json": df_json,
            "plan": selected_plan,
            "messages": []
        }

        execution_results = []
        final_report = ""

        events = execution_graph.stream(initial_state, config={"recursion_limit": 100})
        for event in events:
            for node_name, state_update in event.items():
                if state_update is None:
                    continue
                if node_name == "statistics_expert" and state_update.get('messages'):
                    last_message = state_update['messages'][-1]
                    if isinstance(last_message, ToolMessage):
                        result = json.loads(last_message.content)
                        if 'plot_data' in result and result['plot_data']:
                            chart_base64 = create_plot_base64(result['plot_data'])
                            if chart_base64:
                                result['chart'] = chart_base64
                        execution_results.append(result)
                elif node_name == "reporter" and 'report' in state_update:
                    final_report = state_update['report']

        # ì‘ì—… ì™„ë£Œ í›„ ê²°ê³¼ ì €ì¥
        tasks[task_id].update({
            'status': 'completed',
            'results': execution_results,
            'report': final_report,
            'message': f'{len(execution_results)}ê°œì˜ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'
        })

    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
        tasks[task_id].update({
            'status': 'failed',
            'message': f'ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
        })


# ==============================================================================
# Flask ë¼ìš°íŠ¸ ì •ì˜
# ==============================================================================
@app.route('/')
def index():
    return render_template('index.html')


@app.route('/upload_csv', methods=['POST'])
def upload_csv():
    try:
        file = request.files['file']
        if file and file.filename.endswith('.csv'):
            df = pd.read_csv(file)
            df = df.replace({np.nan: None})
            session['df_json'] = df.to_json(orient='records')
            session['df_info'] = {
                'shape': df.shape,
                'columns': list(df.columns),
                'dtypes': {k: str(v) for k, v in df.dtypes.to_dict().items()}
            }
            preview_data = df.head().to_dict('records')
            return jsonify({
                'success': True,
                'shape': df.shape,
                'columns': list(df.columns),
                'preview': preview_data,
                'message': f'íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ({df.shape[0]}í–‰, {df.shape[1]}ì—´)'
            })
        else:
            return jsonify({'success': False, 'message': 'CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.'})
    except Exception as e:
        return jsonify({'success': False, 'message': f'íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'})


@app.route('/generate_plan', methods=['POST'])
def generate_plan():
    try:
        data = request.json
        api_key = data.get('api_key')
        if not api_key:
            return jsonify({'success': False, 'message': 'OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.'})
        if 'df_json' not in session:
            return jsonify({'success': False, 'message': 'ë¨¼ì € CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.'})

        os.environ["OPENAI_API_KEY"] = api_key
        llm = ChatOpenAI(model="gpt-4o")

        df = pd.read_json(io.StringIO(session['df_json']))
        profile = {
            'shape': df.shape,
            'columns': list(df.columns),
            'dtypes': {k: str(v) for k, v in df.dtypes.to_dict().items()},
            'numeric_columns': df.select_dtypes(include=np.number).columns.tolist(),
            'categorical_columns': df.select_dtypes(include=['object', 'category']).columns.tolist()
        }
        planning_graph = create_graph(llm, for_planning_only=True)
        result = planning_graph.invoke({"data_profile": profile})
        plan = result.get('plan', [])
        session['analysis_plan'] = plan

        return jsonify({
            'success': True,
            'plan': plan,
            'message': f'{len(plan)}ê°œì˜ ë¶„ì„ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.'
        })
    except Exception as e:
        return jsonify({'success': False, 'message': f'ë¶„ì„ ê³„íš ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'})


# ğŸ‘ˆ [ìˆ˜ì •] /execute_analysis ë¼ìš°íŠ¸ ìˆ˜ì •
@app.route('/execute_analysis', methods=['POST'])
def execute_analysis():
    data = request.json
    api_key = data.get('api_key')
    selected_indices = data.get('selected_analyses', [])

    if not api_key:
        return jsonify({'success': False, 'message': 'OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.'})
    if 'df_json' not in session or 'analysis_plan' not in session:
        return jsonify({'success': False, 'message': 'ë¨¼ì € CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ ê³„íšì„ ìƒì„±í•´ì£¼ì„¸ìš”.'})

    full_plan = session['analysis_plan']
    selected_plan = [full_plan[i] for i in selected_indices if i < len(full_plan)]

    if not selected_plan:
        return jsonify({'success': False, 'message': 'ì‹¤í–‰í•  ë¶„ì„ì„ ì„ íƒí•´ì£¼ì„¸ìš”.'})

    # ì‘ì—… ID ìƒì„± ë° ì´ˆê¸° ìƒíƒœ ì„¤ì •
    task_id = str(uuid.uuid4())
    tasks[task_id] = {'status': 'pending'}

    # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ë¶„ì„ ì‹¤í–‰
    thread = threading.Thread(target=run_analysis_in_background, args=(
        task_id,
        api_key,
        selected_plan,
        session['df_json']
    ))
    thread.start()

    # ì‘ì—… IDë¥¼ í´ë¼ì´ì–¸íŠ¸ì— ì¦‰ì‹œ ë°˜í™˜
    return jsonify({'success': True, 'task_id': task_id})


#ì‘ì—… ìƒíƒœë¥¼ ë°˜í™˜í•˜ëŠ” ìƒˆë¡œìš´ ë¼ìš°íŠ¸ ì¶”ê°€
@app.route('/get_task_status/<task_id>')
def get_task_status(task_id):
    """ì‘ì—… IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜„ì¬ ì‘ì—… ìƒíƒœì™€ ê²°ê³¼ë¥¼ ë°˜í™˜"""
    task = tasks.get(task_id, {'status': 'not_found', 'message': 'ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'})
    return jsonify(task)


if __name__ == '__main__':
    app.run(debug=True)
